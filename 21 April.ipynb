{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "01b2cbf4-a7f3-4db1-8565-76da06762f8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Manhattan distance captures the distance between two points by aggregating the pairwise absolute difference between each \\n   variable while Euclidean distance captures the same by aggregating the squared difference in each variable.\\n   We don't use Manhattan Distance, because it calculates distance horizontally or vertically only. It has dimension\\n   restrictions. On the other hand, the Euclidean metric can be used in any space to calculate distance.\\n   Usually, the Euclidean distance is used as the distance metric. Then, it assigns the point to the class among its k \\n   nearest neighbours (where k is an integer).\\n   \\n   The formula to calculate Euclidean distance is: For each dimension, we subtract one point's value from the other's to\\n   get the length of that “side” of the triangle in that dimension, square it, and add it to our running total. The square\\n   root of that running total is our Euclidean distance.\\n   \\n   Manhattan distance is a metric in which the distance between two points is the sum of the absolute differences of their \\n   Cartesian coordinates. In a simple way of saying it is the total sum of the difference between the x-coordinates and\\n   y-coordinates.\\n   \""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Q1\n",
    "'''Manhattan distance captures the distance between two points by aggregating the pairwise absolute difference between each \n",
    "   variable while Euclidean distance captures the same by aggregating the squared difference in each variable.\n",
    "   We don't use Manhattan Distance, because it calculates distance horizontally or vertically only. It has dimension\n",
    "   restrictions. On the other hand, the Euclidean metric can be used in any space to calculate distance.\n",
    "   Usually, the Euclidean distance is used as the distance metric. Then, it assigns the point to the class among its k \n",
    "   nearest neighbours (where k is an integer).\n",
    "   \n",
    "   The formula to calculate Euclidean distance is: For each dimension, we subtract one point's value from the other's to\n",
    "   get the length of that “side” of the triangle in that dimension, square it, and add it to our running total. The square\n",
    "   root of that running total is our Euclidean distance.\n",
    "   \n",
    "   Manhattan distance is a metric in which the distance between two points is the sum of the absolute differences of their \n",
    "   Cartesian coordinates. In a simple way of saying it is the total sum of the difference between the x-coordinates and\n",
    "   y-coordinates.\n",
    "   '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6fc682f5-58cf-40ec-8a50-9edd1b0e0da6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The choice of k will largely depend on the input data as data with more outliers or noise will likely perform better with \\n   higher values of k. Overall, it is recommended to have an odd number for k to avoid ties in classification, and\\n   cross-validation tactics can help you choose the optimal k for your dataset.\\n   \\n   One can use cross-validation to select the optimal value of k for the k-NN algorithm, which helps improve its performance\\n   and prevent overfitting or underfitting.\\n   \\n   The value of k in the KNN algorithm is related to the error rate of the model. A small value of k could lead to overfitting\\n   as well as a big value of k can lead to underfitting. Overfitting imply that the model is well on the training data but has\\n   poor performance when new data is coming.\\n   '"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Q2\n",
    "'''The choice of k will largely depend on the input data as data with more outliers or noise will likely perform better with \n",
    "   higher values of k. Overall, it is recommended to have an odd number for k to avoid ties in classification, and\n",
    "   cross-validation tactics can help you choose the optimal k for your dataset.\n",
    "   \n",
    "   One can use cross-validation to select the optimal value of k for the k-NN algorithm, which helps improve its performance\n",
    "   and prevent overfitting or underfitting.\n",
    "   \n",
    "   The value of k in the KNN algorithm is related to the error rate of the model. A small value of k could lead to overfitting\n",
    "   as well as a big value of k can lead to underfitting. Overfitting imply that the model is well on the training data but has\n",
    "   poor performance when new data is coming.\n",
    "   '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7fdd4caf-f178-4fea-9970-b2549e3b418d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"We mean by the 'best distance metric' (in this review) is the one that allows the KNN to classify test examples with the\\n   highest precision, recall and accuracy, i.e. the one that gives best performance of the KNN in terms of accuracy.\\n   \\n   To classify an unknown instance represented by some feature vectors as a point in the feature space, the k-NN classifier \\n   calculates the distances between the point and points in the training data set. Usually, the Euclidean distance is used as\\n   the distance metric.\\n   \\n   Euclidean Distance – This distance is the most widely used one as it is the default metric that SKlearn library of Python \\n   uses for K-Nearest Neighbour. It is a measure of the true straight line distance between two points in Euclidean space.\\n   \\n   The performance of the K-NN algorithm is influenced by three main factors :\\n1. The distance function or distance metric used to determine the nearest neighbors.\\n2. The decision rule used to derive a classification from the K-nearest neighbors.\\n3. The number of neighbors used to classify.\\n\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Q3\n",
    "'''We mean by the 'best distance metric' (in this review) is the one that allows the KNN to classify test examples with the\n",
    "   highest precision, recall and accuracy, i.e. the one that gives best performance of the KNN in terms of accuracy.\n",
    "   \n",
    "   To classify an unknown instance represented by some feature vectors as a point in the feature space, the k-NN classifier \n",
    "   calculates the distances between the point and points in the training data set. Usually, the Euclidean distance is used as\n",
    "   the distance metric.\n",
    "   \n",
    "   Euclidean Distance – This distance is the most widely used one as it is the default metric that SKlearn library of Python \n",
    "   uses for K-Nearest Neighbour. It is a measure of the true straight line distance between two points in Euclidean space.\n",
    "   \n",
    "   The performance of the K-NN algorithm is influenced by three main factors :\n",
    "1. The distance function or distance metric used to determine the nearest neighbors.\n",
    "2. The decision rule used to derive a classification from the K-nearest neighbors.\n",
    "3. The number of neighbors used to classify.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "06f905ab-da11-4d55-9a58-b1e9fed59001",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The most important hyperparameter for KNN is the number of neighbors (n_neighbors). Test values between at least 1 and 21,\\n   perhaps just the odd numbers. It may also be interesting to test different distance metrics (metric) for choosing the \\n   composition of the neighborhood.\\n   So then hyperparameter optimization is the process of finding the right combination of hyperparameter values to achieve\\n   maximum performance on the data in a reasonable amount of time. This process plays a vital role in the prediction accuracy\\n   of a machine learning algorithm.\\n   \\n   We will use three hyperparamters-\\n   1.n-neighbors,\\n   2.weights and\\n   3.metric.\\n   '"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Q4\n",
    "'''The most important hyperparameter for KNN is the number of neighbors (n_neighbors). Test values between at least 1 and 21,\n",
    "   perhaps just the odd numbers. It may also be interesting to test different distance metrics (metric) for choosing the \n",
    "   composition of the neighborhood.\n",
    "   So then hyperparameter optimization is the process of finding the right combination of hyperparameter values to achieve\n",
    "   maximum performance on the data in a reasonable amount of time. This process plays a vital role in the prediction accuracy\n",
    "   of a machine learning algorithm.\n",
    "   \n",
    "   We will use three hyperparamters-\n",
    "   1.n-neighbors,\n",
    "   2.weights and\n",
    "   3.metric.\n",
    "   '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b6e05db-d78a-4250-8a33-c751e460cf36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'So if dataset is large, there will be a lot of processing which may adversely impact the performance of the algorithm. \\n   KNN is also very sensitive to noise in the dataset. If the dataset is large, there are chances of noise in the dataset\\n   which adversely affect the performance of KNN algorithm.\\n   The steps in rescaling features in KNN are as follows:\\n1. Load the library.\\n2. Load the dataset.\\n3. Sneak Peak Data.\\n4. Standard Scaling.\\n5. Robust Scaling.\\n6. Min-Max Scaling.\\n7. Tuning Hyperparameters.\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Q5\n",
    "'''So if dataset is large, there will be a lot of processing which may adversely impact the performance of the algorithm. \n",
    "   KNN is also very sensitive to noise in the dataset. If the dataset is large, there are chances of noise in the dataset\n",
    "   which adversely affect the performance of KNN algorithm.\n",
    "   The steps in rescaling features in KNN are as follows:\n",
    "1. Load the library.\n",
    "2. Load the dataset.\n",
    "3. Sneak Peak Data.\n",
    "4. Standard Scaling.\n",
    "5. Robust Scaling.\n",
    "6. Min-Max Scaling.\n",
    "7. Tuning Hyperparameters.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "80950dff-1873-423a-8c0a-de473e56fc3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"KNN is sensitive to outliers and missing values and hence we first need to impute the missing values and get rid of the\\n   outliers before applying the KNN algorithm.\\n   Advantages and disadvantages of KNN\\n1. It's easy to understand and simple to implement.\\n2. It can be used for both classification and regression problems.\\n3. It's ideal for non-linear data since there's no assumption about underlying data.\\n4. It can naturally handle multi-class cases.\\n   Some Disadvantages of KNN\\n1. Accuracy depends on the quality of the data.\\n2. With large data, the prediction stage might be slow.\\n3. Sensitive to the scale of the data and irrelevant features.\\n4. Require high memory – need to store all of the training data.\\n5. Given that it stores all of the training, it can be computationally expensive.\\n\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Q6\n",
    "'''KNN is sensitive to outliers and missing values and hence we first need to impute the missing values and get rid of the\n",
    "   outliers before applying the KNN algorithm.\n",
    "   Advantages and disadvantages of KNN\n",
    "1. It's easy to understand and simple to implement.\n",
    "2. It can be used for both classification and regression problems.\n",
    "3. It's ideal for non-linear data since there's no assumption about underlying data.\n",
    "4. It can naturally handle multi-class cases.\n",
    "   Some Disadvantages of KNN\n",
    "1. Accuracy depends on the quality of the data.\n",
    "2. With large data, the prediction stage might be slow.\n",
    "3. Sensitive to the scale of the data and irrelevant features.\n",
    "4. Require high memory – need to store all of the training data.\n",
    "5. Given that it stores all of the training, it can be computationally expensive.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d9c0ba1-a366-43d6-b901-ec3a29b8fadd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
